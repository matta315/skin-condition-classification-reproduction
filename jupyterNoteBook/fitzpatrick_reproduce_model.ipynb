{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7966577b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# coding: utf-8"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea4fc8f7",
   "metadata": {},
   "source": [
    "# 1. Setting Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fd651f6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset, WeightedRandomSampler\n",
    "from torchvision import models, transforms\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix, classification_report, roc_curve, auc, precision_recall_curve\n",
    "from sklearn.preprocessing import label_binarize\n",
    "from PIL import Image\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set seed for reproducibility\n",
    "seed = 42\n",
    "torch.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "# Define paths\n",
    "DATA_PATH = \"../data/processed/images\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5a33a339",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set device to CPI\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e0a59b3",
   "metadata": {},
   "source": [
    "# 2. DataSet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "791666b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FitzpatrickDataset(Dataset):\n",
    "    def __init__(self, csv_file, root_dir, transform=None, target_condition=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            csv_file: Path to the CSV file with annotations\n",
    "            root_dir: Directory with all the images\n",
    "            transform: Optional transform to be applied on a sample\n",
    "            target_condition: If specified, only include this skin condition or list of conditions\n",
    "        \"\"\"\n",
    "        self.data_frame = pd.read_csv(csv_file)\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        \n",
    "        # Verify image paths exist\n",
    "        self.data_frame['image_exists'] = self.data_frame['image_path'].apply(os.path.exists)\n",
    "        self.data_frame = self.data_frame[self.data_frame['image_exists']]\n",
    "        self.data_frame = self.data_frame.drop('image_exists', axis=1)\n",
    "        \n",
    "        # Filter by condition if specified\n",
    "        if target_condition is not None:\n",
    "            if isinstance(target_condition, list):\n",
    "                # Filter for multiple conditions\n",
    "                self.data_frame = self.data_frame[self.data_frame['label'].isin(target_condition)]\n",
    "            else:\n",
    "                # Filter for a single condition\n",
    "                self.data_frame = self.data_frame[self.data_frame['label'] == target_condition]\n",
    "        \n",
    "        # Convert three_partition_label to binary (malignant vs. non-malignant)\n",
    "        self.data_frame['binary_label'] = self.data_frame['three_partition_label'].apply(\n",
    "            lambda x: 1 if x == 'malignant' else 0\n",
    "        )\n",
    "        \n",
    "        # Group skin types into light (1-3) and dark (4-6)\n",
    "        self.data_frame['skin_group'] = self.data_frame['fitzpatrick_scale'].apply(\n",
    "            lambda x: 0 if x <= 3 else 1  # 0 for light, 1 for dark\n",
    "        )\n",
    "        \n",
    "        # Create a mapping for unique conditions\n",
    "        self.unique_conditions = self.data_frame['label'].unique()\n",
    "        self.condition_to_idx = {condition: idx for idx, condition in enumerate(self.unique_conditions)}\n",
    "        \n",
    "        # Add multi-class label\n",
    "        self.data_frame['condition_idx'] = self.data_frame['label'].apply(lambda x: self.condition_to_idx[x])\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data_frame)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "            \n",
    "        img_path = self.data_frame.iloc[idx]['image_path']\n",
    "        \n",
    "        try:\n",
    "            # Use PIL to load image\n",
    "            image = Image.open(img_path).convert('RGB')\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading image {img_path}: {e}\")\n",
    "            # Return a placeholder image if loading fails\n",
    "            image = Image.new('RGB', (224, 224), color='gray')\n",
    "        \n",
    "        # Get labels\n",
    "        binary_label = float(self.data_frame.iloc[idx]['binary_label'])  # Convert to float\n",
    "        skin_type = int(self.data_frame.iloc[idx]['fitzpatrick_scale'])\n",
    "        skin_group = int(self.data_frame.iloc[idx]['skin_group'])\n",
    "        condition_idx = int(self.data_frame.iloc[idx]['condition_idx'])\n",
    "        \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        \n",
    "        # Return a simple tuple instead of a dictionary\n",
    "        return image, binary_label\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9a3393c",
   "metadata": {},
   "source": [
    "# 3. Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e1a31e2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(batch_size=32, target_condition=None, max_samples=None):\n",
    "    \"\"\"\n",
    "    Load and prepare the datasets with optimization for speed\n",
    "    \"\"\"\n",
    "    # Define transforms with smaller image size\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((128, 128)),  # Smaller size for faster training\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])  # ImageNet normalization\n",
    "    ])\n",
    "    \n",
    "    # Create datasets\n",
    "    train_dataset = FitzpatrickDataset(\n",
    "        csv_file=os.path.join(DATA_PATH, 'train_split.csv'),\n",
    "        root_dir=DATA_PATH,\n",
    "        transform=transform,\n",
    "        target_condition=target_condition\n",
    "    )\n",
    "    \n",
    "    val_dataset = FitzpatrickDataset(\n",
    "        csv_file=os.path.join(DATA_PATH, 'val_split.csv'),\n",
    "        root_dir=DATA_PATH,\n",
    "        transform=transform,\n",
    "        target_condition=target_condition\n",
    "    )\n",
    "    \n",
    "    test_dataset = FitzpatrickDataset(\n",
    "        csv_file=os.path.join(DATA_PATH, 'test_split.csv'),\n",
    "        root_dir=DATA_PATH,\n",
    "        transform=transform,\n",
    "        target_condition=target_condition\n",
    "    )\n",
    "    \n",
    "    # Optionally limit the number of samples\n",
    "    if max_samples is not None:\n",
    "        # Simple random sampling for testing\n",
    "        if len(train_dataset) > max_samples:\n",
    "            indices = torch.randperm(len(train_dataset))[:max_samples]\n",
    "            train_dataset = torch.utils.data.Subset(train_dataset, indices)\n",
    "        if len(val_dataset) > max_samples // 5:\n",
    "            indices = torch.randperm(len(val_dataset))[:max_samples // 5]\n",
    "            val_dataset = torch.utils.data.Subset(val_dataset, indices)\n",
    "        if len(test_dataset) > max_samples // 5:\n",
    "            indices = torch.randperm(len(test_dataset))[:max_samples // 5]\n",
    "            test_dataset = torch.utils.data.Subset(test_dataset, indices)\n",
    "    \n",
    "    # Create dataloaders with fewer workers and persistent_workers=False\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset, \n",
    "        batch_size=batch_size, \n",
    "        shuffle=True,\n",
    "        num_workers=0,  # Use 0 workers to debug\n",
    "        pin_memory=True\n",
    "    )\n",
    "    \n",
    "    val_loader = DataLoader(\n",
    "        val_dataset, \n",
    "        batch_size=batch_size, \n",
    "        shuffle=False,\n",
    "        num_workers=0,  # Use 0 workers to debug\n",
    "        pin_memory=True\n",
    "    )\n",
    "    \n",
    "    test_loader = DataLoader(\n",
    "        test_dataset, \n",
    "        batch_size=batch_size, \n",
    "        shuffle=False,\n",
    "        num_workers=0,  # Use 0 workers to debug\n",
    "        pin_memory=True\n",
    "    )\n",
    "    \n",
    "    # Get unique conditions from the original dataset (before subsetting)\n",
    "    if hasattr(train_dataset, 'unique_conditions'):\n",
    "        unique_conditions = train_dataset.unique_conditions\n",
    "    elif hasattr(train_dataset, 'dataset') and hasattr(train_dataset.dataset, 'unique_conditions'):\n",
    "        unique_conditions = train_dataset.dataset.unique_conditions\n",
    "    else:\n",
    "        # If we can't get unique conditions directly, try to extract from the dataframe\n",
    "        try:\n",
    "            if hasattr(train_dataset, 'data_frame'):\n",
    "                unique_conditions = train_dataset.data_frame['label'].unique()\n",
    "            elif hasattr(train_dataset, 'dataset') and hasattr(train_dataset.dataset, 'data_frame'):\n",
    "                unique_conditions = train_dataset.dataset.data_frame['label'].unique()\n",
    "            else:\n",
    "                unique_conditions = []\n",
    "        except:\n",
    "            unique_conditions = []\n",
    "    \n",
    "    return train_loader, val_loader, test_loader, unique_conditions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "81bddab9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a smaller dataset with focus on key conditions\n",
    "target_conditions = ['psoriasis', 'squamous_cell_carcinoma', 'lichen_planus']\n",
    "train_loader, val_loader, test_loader, unique_conditions = load_data(\n",
    "    batch_size=32,\n",
    "    target_condition=target_conditions,\n",
    "    max_samples=1000  # Limit total samples\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6ca43ac",
   "metadata": {},
   "source": [
    "# 4. Model Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "985cd9d7",
   "metadata": {},
   "source": [
    "Using ResNet 18 instead of VGG16 as paper for faster training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b66c7c91",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResNet18Classifier(nn.Module):\n",
    "    def __init__(self, num_classes=2, pretrained=True):\n",
    "        \"\"\"\n",
    "        ResNet18-based classifier that can be used for binary or multi-class classification\n",
    "        \n",
    "        Args:\n",
    "            num_classes: Number of output classes\n",
    "            pretrained: Whether to use pretrained weights from ImageNet\n",
    "        \"\"\"\n",
    "        super(ResNet18Classifier, self).__init__()\n",
    "        \n",
    "        # Load pre-trained ResNet18 model\n",
    "        self.model = models.resnet18(pretrained=pretrained)\n",
    "        \n",
    "        # Replace the final fully connected layer for our classification task\n",
    "        num_features = self.model.fc.in_features\n",
    "        self.model.fc = nn.Linear(num_features, num_classes)\n",
    "        \n",
    "        # Initialize the new layer\n",
    "        nn.init.normal_(self.model.fc.weight, 0, 0.01)\n",
    "        nn.init.constant_(self.model.fc.bias, 0)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5618d066",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create model function\n",
    "def create_multiclass_model(num_classes):\n",
    "    \"\"\"\n",
    "    Create a multi-class classifier model for specific skin conditions\n",
    "    \"\"\"\n",
    "    model = ResNet18Classifier(num_classes=num_classes)\n",
    "    return model\n",
    "\n",
    "# Initialize model\n",
    "num_classes = len(unique_conditions)\n",
    "model = create_multiclass_model(num_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50938622",
   "metadata": {},
   "source": [
    "# 5. Training Function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cd17d65",
   "metadata": {},
   "source": [
    "Including mixed precision for faster training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2de99cc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, val_loader, criterion, optimizer, num_epochs=5, device='cpu'):\n",
    "    \"\"\"\n",
    "    Train the model\n",
    "    \"\"\"\n",
    "    # Initialize lists to track metrics\n",
    "    history = {\n",
    "        'train_loss': [], 'train_acc': [],\n",
    "        'val_loss': [], 'val_acc': []\n",
    "    }\n",
    "    \n",
    "    # Training loop\n",
    "    for epoch in range(num_epochs):\n",
    "        print(f'Epoch {epoch+1}/{num_epochs}')\n",
    "        print('-' * 10)\n",
    "        \n",
    "        # Training phase\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        running_corrects = 0\n",
    "        total = 0\n",
    "        \n",
    "        for inputs, labels in train_loader:\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.float().to(device).view(-1, 1)\n",
    "            \n",
    "            # Zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Forward pass\n",
    "            with torch.set_grad_enabled(True):\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "                \n",
    "                # Backward pass and optimize\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "            \n",
    "            # Statistics\n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "            preds = (outputs > 0.5).float()\n",
    "            running_corrects += torch.sum(preds == labels).item()\n",
    "            total += labels.size(0)\n",
    "        \n",
    "        epoch_loss = running_loss / total\n",
    "        epoch_acc = running_corrects / total\n",
    "        \n",
    "        history['train_loss'].append(epoch_loss)\n",
    "        history['train_acc'].append(epoch_acc)\n",
    "        \n",
    "        print(f'Train Loss: {epoch_loss:.4f} Acc: {epoch_acc:.4f}')\n",
    "        \n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        running_loss = 0.0\n",
    "        running_corrects = 0\n",
    "        total = 0\n",
    "        \n",
    "        for inputs, labels in val_loader:\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.float().to(device).view(-1, 1)\n",
    "            \n",
    "            # Forward pass\n",
    "            with torch.no_grad():\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "            \n",
    "            # Statistics\n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "            preds = (outputs > 0.5).float()\n",
    "            running_corrects += torch.sum(preds == labels).item()\n",
    "            total += labels.size(0)\n",
    "        \n",
    "        epoch_loss = running_loss / total\n",
    "        epoch_acc = running_corrects / total\n",
    "        \n",
    "        history['val_loss'].append(epoch_loss)\n",
    "        history['val_acc'].append(epoch_acc)\n",
    "        \n",
    "        print(f'Val Loss: {epoch_loss:.4f} Acc: {epoch_acc:.4f}')\n",
    "    \n",
    "    return model, history\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4654797f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "----------\n",
      "Train Loss: 0.0000 Acc: 0.9714\n",
      "Val Loss: 0.0000 Acc: 0.7895\n",
      "Epoch 2/5\n",
      "----------\n",
      "Train Loss: 0.0000 Acc: 0.9714\n",
      "Val Loss: 0.0000 Acc: 0.8421\n",
      "Epoch 3/5\n",
      "----------\n",
      "Train Loss: 0.0000 Acc: 0.9857\n",
      "Val Loss: 0.0000 Acc: 0.8947\n",
      "Epoch 4/5\n",
      "----------\n",
      "Train Loss: 0.0000 Acc: 0.9714\n",
      "Val Loss: 0.0000 Acc: 0.8947\n",
      "Epoch 5/5\n",
      "----------\n",
      "Train Loss: 0.0000 Acc: 0.9714\n",
      "Val Loss: 0.0000 Acc: 0.9474\n"
     ]
    }
   ],
   "source": [
    "# Define loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0001)\n",
    "\n",
    "# Train model\n",
    "num_epochs = 5\n",
    "model, history = train_model(model, train_loader, val_loader, criterion, optimizer, num_epochs=num_epochs, device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e01df2ff",
   "metadata": {},
   "source": [
    "# 6. MODEL VALIDATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5802845e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_model_across_skin_types(model, test_loader, device='cpu'):\n",
    "    \"\"\"\n",
    "    Evaluate model performance across different Fitzpatrick skin types\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    all_skin_types = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for images, labels, skin_types, _, _ in test_loader:\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            outputs = model(images)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            \n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            all_skin_types.extend(skin_types.numpy())\n",
    "    \n",
    "    # Convert to numpy arrays\n",
    "    all_preds = np.array(all_preds)\n",
    "    all_labels = np.array(all_labels)\n",
    "    all_skin_types = np.array(all_skin_types)\n",
    "    \n",
    "    # Calculate overall accuracy\n",
    "    overall_accuracy = np.mean(all_preds == all_labels)\n",
    "    print(f\"Overall Accuracy: {overall_accuracy:.4f}\")\n",
    "    \n",
    "    # Calculate accuracy by skin type\n",
    "    print(\"\\nAccuracy by Fitzpatrick Skin Type:\")\n",
    "    skin_type_metrics = {}\n",
    "    for skin_type in range(1, 7):  # Fitzpatrick types 1-6\n",
    "        mask = all_skin_types == skin_type\n",
    "        if np.sum(mask) > 0:  # Only calculate if we have samples of this skin type\n",
    "            skin_type_acc = np.mean(all_preds[mask] == all_labels[mask])\n",
    "            skin_type_metrics[skin_type] = skin_type_acc\n",
    "            print(f\"  Type {skin_type}: {skin_type_acc:.4f} (n={np.sum(mask)})\")\n",
    "    \n",
    "    # Calculate accuracy for light vs dark skin\n",
    "    light_mask = all_skin_types <= 3\n",
    "    dark_mask = all_skin_types >= 4\n",
    "    light_acc = np.mean(all_preds[light_mask] == all_labels[light_mask])\n",
    "    dark_acc = np.mean(all_preds[dark_mask] == all_labels[dark_mask])\n",
    "    \n",
    "    print(\"\\nAccuracy by Skin Group:\")\n",
    "    print(f\"  Light Skin (Types 1-3): {light_acc:.4f} (n={np.sum(light_mask)})\")\n",
    "    print(f\"  Dark Skin (Types 4-6): {dark_acc:.4f} (n={np.sum(dark_mask)})\")\n",
    "    \n",
    "    return {\n",
    "        'overall_accuracy': overall_accuracy,\n",
    "        'skin_type_metrics': skin_type_metrics,\n",
    "        'light_skin_accuracy': light_acc,\n",
    "        'dark_skin_accuracy': dark_acc\n",
    "    }\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
